from app import app
from flask import Flask, render_template, flash

import json
import math

# from ner import stanfordNE2BIO, geonames_query, stanfordNE2tree
# from places import places_dict
from constants import USERNAME, BASE_URL, PAPERS_URLS
# import os
import requests
from lxml import html
# import re
from nameparser import HumanName
# from tqdm import tqdm #for progress bar
# from lxml.etree import tostring
#
# import xml.etree.ElementTree as ET

# import requests
# import random
#
# from nltk.tag import StanfordNERTagger
# from nltk.tokenize import word_tokenize
# from nltk.tree import Tree
#
# import folium
#
# from pprint import pprint
# from tqdm import tqdm #for progress bar
# from flask_cors import CORS
# from lxml.etree import tostring
#
# # app = Flask(__name__)
# # CORS(app)
# #
# # app.debug = True
# # # app.config.from_object(os.environ['APP_SETTINGS'])
#
#


# Helper functions
def _sort_names(names):
    # Should check for 'last' in keys
    parsed_names = [HumanName(name) for name in names]
    return [' '.join(name) for name in sorted(parsed_names, key=lambda x: x['last'])]


# Routes
@app.route('/')
def homepage():
    """
    Route to the homepage
    """
    authors_data = dict()
    for i, url in enumerate(PAPERS_URLS, 1):
        with open("data/papers/isaw-papers-%s.xhtml" % i, "r") as paper:
            html_content = html.parse(paper)
        a1 = html_content.xpath('//span[@rel="dcterms:creator"]//text()')
        a2 = html_content.xpath('//span[contains(@property, "dcterms:creator")]/text()')
        a3 = html_content.xpath('//h2[contains(@property, "dcterms:creator")]/text()')
        a = _sort_names(list(set(a1+a2+a3)))
        authors_data[str(i)] = a
    return render_template('index.html',  authors_data=authors_data, BASE_URL=BASE_URL)


@app.route('/authors')
def get_papers():
    """
    Route to a page listing the papers by authors
    """
    authors_papers = dict()
    for i, url in enumerate(PAPERS_URLS, 1):
        with open("data/papers/isaw-papers-%s.xhtml" % i, "r") as paper:
            html_content = html.parse(paper)
        authors = html_content.xpath('//span[@rel="dcterms:creator"]//text()')
        authors += html_content.xpath('//span[contains(@property, "dcterms:creator")]/text()')
        authors += html_content.xpath('//h2[contains(@property, "dcterms:creator")]/text()')
        authors = list(set(_sort_names(authors)))
        for author in authors:
            try:
                authors_papers[author]["articles"].append(str(i))
            except KeyError:
                try:
                    authors_papers[author]["article"] = list()
                except KeyError:
                    authors_papers[author] = dict()
                    authors_papers[author]["articles"] = list()
                authors_papers[author]["articles"].append(str(i))
            authors_papers[author]["viaf"] = html_content.xpath('//span[@rel="dcterms:creator"]/a[.="%s"]/@href' % author)
    return render_template('author_reversed.html', authors_papers=authors_papers, BASE_URL=BASE_URL)


@app.route('/places')
def get_places():
    """
    Route to a page listing the places mentionned in the papers
    """
    places_data = dict()
    for i, url in enumerate(PAPERS_URLS, 1):
        with open("data/papers/isaw-papers-%s.xhtml" % i, "r") as paper:
            html_content = html.parse(paper)
        place_name = html_content.xpath('//a[starts-with(@href,"https://pleiades.stoa.org/place")]/text()')
        places_data[str(i)] = dict()
        with open("data/places.json", "r") as places_json:
            places_article = json.load(places_json)
        for name in place_name:
            if name in places_article.keys():
                places_data[str(i)][name] = places_article[name][:3]
    return render_template('places.html', places_data=places_data)


@app.route('/map/<article_id>')
@app.route('/<article_id>/map')
@app.route('/map')
def map_places(**kwargs):
    """
    Route to a pages with a map of all the places mentionned in the articles (one of the paper if the number is give as an argument)
    """
    if kwargs:

        i = kwargs["article_id"]
        places = dict()
        with open("data/places.json", "r") as places_json:
            places_article = json.load(places_json)
        for k, v in places_article.items():
            if places_article[k][3] == [str(i)]:
                places[k] = v
        article = "ISAW Papers " + str(i)
    else:
        article = "ISAW Papers"
        places = dict()
        with open("data/places.json", "r") as places_json:
            places_article = json.load(places_json)
            for k, v in places_article.items():
                if k in places:
                    for p in places_article[k][2]:
                        places[k][2].append(p)
                        if str(i) not in places[k][3]:
                            places[k][3].append(str(i))
                    places[k][3] = ' and '.join(places[k][3])
                else:
                    places[k] = v
    for k, v in places.items():
        # size of the circle on the map (places[k][5])
        radius = math.log10(len(places[k][2]) + 1) * 150000
        places[k].append(radius)
        if type(places[k][3]) is list:
            places[k][3] = ''.join(places[k][3])

    if not places:
        flash("We do not have any places associated with that article", "warning")
    return render_template('map.html', places=places, article=article)
#
# st = StanfordNERTagger('./stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',
#                        './stanford-ner/stanford-ner.jar',
#
#                        encoding='utf-8')
#
#
#
#
#
#
# @app.route('/ner')
# def ner():
#     st = StanfordNERTagger('./stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',
#                            './stanford-ner/stanford-ner.jar',
#
#                            encoding='utf-8')
#
#     with open("data/papers/isaw-papers-%s.xhtml" % (1), "r") as paper:
#         html_parsed = html.parse(paper)
#         work_cited = html_parsed.xpath('//p[@class="work_cited"]')
#         html_parsed = [html_parsed]
#
#
#     places_set = []
#     for html_c in tqdm(html_parsed):
#         html_c = tostring(html_c, encoding="unicode")
#         for work in work_cited :
#             work = tostring(work, encoding="unicode")
#             html_c = html_c.replace(work, "")
#
#         tokenized_marc = word_tokenize(html_c)
#         classified_marc = st.tag(tokenized_marc)
#         classified_marc = [item for item in classified_marc if item[0] != '']  # clean up parsing
#         ne_tree = stanfordNE2tree(classified_marc)
#
#         ne_in_sent = []
#         for subtree in ne_tree:
#             if type(subtree) == Tree:  # If subtree is a noun chunk, i.e. NE != "O"
#                 ne_label = subtree.label()
#                 ne_string = " ".join([token for token, pos in subtree.leaves()])
#                 ne_in_sent.append((ne_string, ne_label))
#
#         locations = set([tag[0] for tag in ne_in_sent if
#                          tag[1] == 'LOCATION'])  # If we don't make this a set, we can use frequency info for map weight
#
#         places_set.append(locations)
#
#     places_set = [sorted(item) for item in places_set]
#
#     # Build list of likely coordinates for places
#
#     places_list = []
#     map = dict()
#
#     for places in places_set:
#         for place in places:
#             ll = geonames_query(place)
#             if ll != ():
#                 places_list.append(place)
#                 ll = list(ll)
#                 map[place] = list()
#                 for l in ll :
#                     map[place].append((float(l)))
#     print(len(map))
#
#     return render_template('map_ner.html', places=map)


@app.route('/tfidf')
def tfidf():
    """ Route displaying the 10 words of each articles with the highest tf-idf score
    """
    number_doc_containing = {}
    tf_idf = {}
    wordcount = {}
    total_word = {}
    for i, url in enumerate(PAPERS_URLS, 1):
        with open("data/papers/isaw-papers-%s.xhtml" % i, "r") as paper:
            html_page = html.parse(paper)
            # deleting the bibliography
            work_cited = html_page.xpath('//p[@class="work_cited"]')
            for work in work_cited:
                work.getparent().remove(work)
            text_page = html_page.xpath("//body//text()")
        total_word[str(i)] = 0
        wordcount[str(i)] = {}
        tf_idf[str(i)] = {}
        # To eliminate duplicates, remember to split by punctuation
        for words in text_page:
            for word in words.lower().split():
                word = word.replace(".", "")
                word = word.replace(",", "")
                word = word.replace(":", "")
                word = word.replace("\"", "")
                word = word.replace("!", "")
                word = word.replace(";", "")
                word = word.replace("(", "")
                word = word.replace(")", "")
                word = word.replace("|", "")
                word = word.replace("â€œ", "")
                word = word.replace("â€˜", "")
                word = word.replace("*", "")

                # counting the occurrences of the words in each document and the number of doc where each word appears
                if word not in wordcount[str(i)]:
                    wordcount[str(i)][word] = 1
                    if word in number_doc_containing.keys():
                        number_doc_containing[word] += 1
                    else:
                        number_doc_containing[word] = 1

                else:
                    wordcount[str(i)][word] += 1
            total_word[str(i)] += 1

    # calculating df and idf scores
    for i, url in enumerate(PAPERS_URLS, 1):
        for word, number in wordcount[str(i)].items():
            tf = wordcount[str(i)][word]/total_word[str(i)]

            tf_idf[str(i)][word] = tf * math.log(i/number_doc_containing[word])
        # put the dictionary in the right order
        tf_idf[str(i)] = sorted(tf_idf[str(i)].items(), key=lambda x: x[1])

        # Taking the ten last values (best if-idf score
        tf_idf[str(i)] = tf_idf[str(i)][-10:]
    return render_template('tfidf.html', tf_idf=tf_idf)
